/**
 * research.ts — Delivers on-demand research reports for AgentLaunch tokens
 *
 * Platform constants (source of truth: deployed smart contracts):
 *   - Deploy fee: 120 FET (read dynamically, can change via multi-sig)
 *   - Graduation target: 30,000 FET -> auto DEX listing
 *   - Trading fee: 2% -> 100% to protocol treasury (NO creator fee)
 */

import type { AgentTemplate } from "../registry.js";

// Environment-based URL resolution
const DEV_API_URL = 'https://launchpad-backend-dev-1056182620041.us-central1.run.app';
const PROD_API_URL = 'https://agent-launch.ai/api';
const RESOLVED_API_URL = process.env.AGENT_LAUNCH_API_URL ??
  (process.env.AGENT_LAUNCH_ENV === 'production' ? PROD_API_URL : DEV_API_URL);

export const template: AgentTemplate = {
  name: "research",
  description:
    "Delivers on-demand research reports and analysis for AgentLaunch tokens",
  category: "AI/ML",
  variables: [
    { name: "agent_name", required: true, description: "Name of the agent" },
    {
      name: "description",
      default: "Delivers on-demand research reports and analysis for AgentLaunch tokens",
      description: "Short description of what this agent does",
    },
    {
      name: "ai_model",
      default: "mistralai/Mistral-7B-Instruct-v0.2",
      description: "Hugging Face model ID for report generation",
    },
    {
      name: "report_max_tokens",
      default: "512",
      description: "Maximum tokens in generated report",
    },
    {
      name: "rate_limit_per_minute",
      default: "10",
      description: "Max requests per user per minute (lower due to AI calls)",
    },
    {
      name: "free_requests_per_day",
      default: "5",
      description: "Free tier daily request quota",
    },
    {
      name: "premium_token_threshold",
      default: "1000",
      description: "Token balance required for premium tier",
    },
  ],
  dependencies: ["requests"],
  secrets: [
    "AGENTVERSE_API_KEY",
    "AGENTLAUNCH_API_KEY",
    "AGENT_ADDRESS",
    "AGENT_OWNER_ADDRESS",
    "HUGGINGFACE_API_KEY",
  ],
  code: `#!/usr/bin/env python3
"""
{{agent_name}} — AgentLaunch Research Agent
Generated by: agentlaunch scaffold {{agent_name}} --type research

Fetches token data from agent-launch.ai and generates a structured research
report using an AI model via the Hugging Face Inference API.

Platform constants (source of truth: deployed smart contracts):
  - Deploy fee: 120 FET (read dynamically, can change via multi-sig)
  - Graduation target: 30,000 FET -> auto DEX listing
  - Trading fee: 2% -> 100% to protocol treasury (NO creator fee)
"""

from uagents import Agent, Context, Protocol
from uagents_core.contrib.protocols.chat import (
    ChatAcknowledgement,
    ChatMessage,
    EndSessionContent,
    TextContent,
    chat_protocol_spec,
)

import json
import os
import time
from collections import defaultdict
from datetime import datetime
from typing import Any, Dict, List, Optional
from uuid import uuid4

import requests

# ==============================================================================
# API CONFIG — Override via environment variables, never hardcode
# ==============================================================================

AGENTLAUNCH_API = os.environ.get("AGENTLAUNCH_API", "${RESOLVED_API_URL}")
HF_API_URL = "https://api-inference.huggingface.co/models/{{ai_model}}"

# ==============================================================================
# BUSINESS CONFIG
# ==============================================================================

OWNER_ADDRESS = os.environ.get("AGENT_OWNER_ADDRESS", "")

BUSINESS = {
    "name": "{{agent_name}}",
    "description": "{{description}}",
    "version": "1.0.0",
    "ai_model": "{{ai_model}}",
    "report_max_tokens": int("{{report_max_tokens}}"),
    "free_requests_per_day": {{free_requests_per_day}},
    "premium_token_threshold": {{premium_token_threshold}},
    "rate_limit_per_minute": {{rate_limit_per_minute}},
    "max_input_length": 5000,
}


# ==============================================================================
# LAYER 1: FOUNDATION
# ==============================================================================


class Logger:
    """Structured logging with audit trail."""

    @staticmethod
    def info(ctx: Context, event: str, data: Optional[Dict] = None) -> None:
        ctx.logger.info(f"[{event}] {json.dumps(data or {})}")

    @staticmethod
    def audit(ctx: Context, user: str, action: str) -> None:
        ctx.logger.info(
            f"[AUDIT] user={user[:20]} action={action} "
            f"ts={datetime.utcnow().isoformat()}"
        )

    @staticmethod
    def error(ctx: Context, event: str, error: str) -> None:
        ctx.logger.error(f"[{event}] {error}")


# ==============================================================================
# LAYER 2: SECURITY
# ==============================================================================


class Security:
    """Rate limiting and input validation."""

    def __init__(self) -> None:
        self._requests: Dict[str, List[float]] = defaultdict(list)
        self._check_count: int = 0

    def check(self, ctx: Context, user_id: str, message: str) -> tuple:
        now = time.time()

        self._requests[user_id] = [
            t for t in self._requests[user_id] if now - t < 60
        ]
        if len(self._requests[user_id]) >= BUSINESS["rate_limit_per_minute"]:
            return None, "Rate limit exceeded. Please wait a moment."
        self._requests[user_id].append(now)

        self._check_count += 1
        if self._check_count % 100 == 0:
            stale = [
                k
                for k, v in self._requests.items()
                if not v or (now - max(v)) > 300
            ]
            for k in stale:
                del self._requests[k]

        if not message or not message.strip():
            return None, "Empty message."
        if len(message) > BUSINESS["max_input_length"]:
            return None, f"Message too long (max {BUSINESS['max_input_length']} chars)."

        return message.strip(), None


# ==============================================================================
# LAYER 3: STABILITY
# ==============================================================================


class Health:
    """Track uptime and error rate."""

    def __init__(self) -> None:
        self._start: datetime = datetime.utcnow()
        self._requests: int = 0
        self._errors: int = 0

    def record(self, success: bool) -> None:
        self._requests += 1
        if not success:
            self._errors += 1

    def status(self) -> Dict[str, Any]:
        uptime = (datetime.utcnow() - self._start).total_seconds()
        error_rate = (self._errors / self._requests * 100) if self._requests else 0
        return {
            "status": "healthy" if error_rate < 10 else "degraded",
            "uptime_seconds": int(uptime),
            "requests": self._requests,
            "error_rate": f"{error_rate:.1f}%",
        }


# ==============================================================================
# LAYER 4: SPEED
# ==============================================================================


class Cache:
    """In-memory TTL cache."""

    def __init__(self, max_size: int = 1000) -> None:
        self._data: Dict[str, tuple] = {}
        self._max_size: int = max_size

    def get(self, key: str) -> Any:
        if key in self._data:
            value, expires = self._data[key]
            if expires > time.time():
                return value
            del self._data[key]
        return None

    def set(self, key: str, value: Any, ttl: int = 300) -> None:
        if len(self._data) >= self._max_size:
            now = time.time()
            expired = [k for k, (_, exp) in self._data.items() if exp <= now]
            for k in expired:
                del self._data[k]
            if len(self._data) >= self._max_size:
                to_drop = sorted(self._data.items(), key=lambda x: x[1][1])[
                    : self._max_size // 10
                ]
                for k, _ in to_drop:
                    del self._data[k]
        self._data[key] = (value, time.time() + ttl)


# ==============================================================================
# LAYER 5: REVENUE
# ==============================================================================


class Revenue:
    """Token-gated access and daily usage quotas."""

    def __init__(self, cache: Cache) -> None:
        self._cache = cache
        self._usage: Dict[str, List[str]] = defaultdict(list)

    def get_tier(self, user_address: str) -> str:
        cached = self._cache.get(f"tier:{user_address}")
        if cached is not None:
            return cached
        try:
            r = requests.get(
                f"{AGENTLAUNCH_API}/agents/token/{user_address}", timeout=5
            )
            if r.status_code == 200:
                data = r.json()
                balance = data.get("balance", 0)
                tier = (
                    "premium"
                    if balance >= BUSINESS["premium_token_threshold"]
                    else "free"
                )
                self._cache.set(f"tier:{user_address}", tier, ttl=300)
                return tier
        except Exception:
            pass
        return "free"

    def check_quota(self, user_id: str, tier: str) -> tuple:
        today = datetime.utcnow().date().isoformat()
        self._usage[user_id] = [
            t for t in self._usage[user_id] if t.startswith(today)
        ]
        today_usage = len(self._usage[user_id])
        limit = 1000 if tier == "premium" else BUSINESS["free_requests_per_day"]
        if today_usage >= limit:
            if tier == "free":
                return False, (
                    f"Free limit reached ({limit}/day). "
                    f"Hold {BUSINESS['premium_token_threshold']} tokens for premium!"
                )
            return False, f"Daily limit reached ({limit}/day)."
        self._usage[user_id].append(datetime.utcnow().isoformat())
        return True, None


# ==============================================================================
# AGENTLAUNCH INTEGRATION
# ==============================================================================


class AgentLaunch:
    """Create and manage tokens on AgentLaunch."""

    @staticmethod
    def tokenize() -> Dict:
        agent_address = os.environ.get("AGENT_ADDRESS")
        if not agent_address:
            return {"error": "AGENT_ADDRESS env var not set."}
        try:
            r = requests.post(
                f"{AGENTLAUNCH_API}/agents/tokenize",
                headers={
                    "X-API-Key": os.environ.get("AGENTLAUNCH_API_KEY", ""),
                    "Content-Type": "application/json",
                },
                json={
                    "agentAddress": agent_address,
                    "name": BUSINESS["name"],
                    "description": BUSINESS["description"],
                },
                timeout=30,
            )
            return r.json() if r.status_code in [200, 201] else {"error": r.text}
        except Exception as e:
            return {"error": str(e)}


# ==============================================================================
# RESEARCH BUSINESS LOGIC
# ==============================================================================


class ResearchBusiness:
    """
    Generates structured research reports for AgentLaunch tokens.

    Steps:
      1. Fetch token metadata from agent-launch.ai/api
      2. Build a research prompt with the on-chain data
      3. Call Hugging Face Inference API for AI-generated analysis
      4. Return the formatted report

    Note: Trading fee is 2% -> 100% to protocol treasury (no creator fee).
    """

    def __init__(self, cache: Cache) -> None:
        self._cache = cache

    def fetch_token(self, address: str) -> Optional[Dict]:
        cache_key = f"token:{address}"
        cached = self._cache.get(cache_key)
        if cached is not None:
            return cached
        try:
            r = requests.get(
                f"{AGENTLAUNCH_API}/agents/token/{address}", timeout=10
            )
            if r.status_code == 200:
                data = r.json()
                token = data.get("data") or data
                if isinstance(token, dict):
                    self._cache.set(cache_key, token, ttl=300)
                    return token
        except Exception:
            pass
        return None

    def generate_report(self, token: Dict, topic: str) -> str:
        """Call Hugging Face Inference API to generate an AI research report."""
        hf_key = os.environ.get("HUGGINGFACE_API_KEY", "")
        if not hf_key:
            return "(AI report unavailable — set HUGGINGFACE_API_KEY to enable)"

        name = token.get("name", "Unknown")
        symbol = token.get("symbol", "???")
        price = token.get("price") or token.get("currentPrice") or 0
        market_cap = token.get("marketCap") or token.get("market_cap") or 0
        description = token.get("description", "No description provided.")[:300]
        listed = token.get("listed") or token.get("isListed") or False

        prompt = (
            f"Write a concise research report for the following AI agent token on the "
            f"AgentLaunch platform.\\n\\n"
            f"Token: {name} ({symbol})\\n"
            f"Description: {description}\\n"
            f"Current price: {float(price):.6f} FET\\n"
            f"Market cap: {float(market_cap):,.0f} FET\\n"
            f"DEX listed: {'Yes' if listed else 'No (graduation target: 30,000 FET)'}\\n"
            f"Platform fee: 2% trading fee (100% to protocol treasury)\\n\\n"
            f"Topic: {topic}\\n\\n"
            f"Report:"
        )

        cache_key = f"report:{name}:{topic[:50]}"
        cached = self._cache.get(cache_key)
        if cached:
            return cached

        try:
            r = requests.post(
                HF_API_URL,
                headers={"Authorization": f"Bearer {hf_key}"},
                json={
                    "inputs": prompt,
                    "parameters": {
                        "max_new_tokens": BUSINESS["report_max_tokens"],
                        "temperature": 0.7,
                    },
                },
                timeout=30,
            )
            if r.status_code == 200:
                result = r.json()
                if isinstance(result, list) and result:
                    text = result[0].get("generated_text", "")
                    # Strip the prompt from the response
                    report = text[len(prompt):].strip() if text.startswith(prompt) else text.strip()
                    self._cache.set(cache_key, report, ttl=1800)
                    return report
        except Exception:
            pass
        return "Could not generate AI report. Please try again later."

    async def handle(self, ctx: Context, user_id: str, message: str, tier: str) -> str:
        lower = message.lower()

        # "report <address> [topic]"
        if lower.startswith("report "):
            parts = message.split(None, 2)
            if len(parts) < 2:
                return "Usage: report <token_address> [topic]"
            address = parts[1].strip()
            topic = parts[2].strip() if len(parts) >= 3 else "overview"

            token = self.fetch_token(address)
            if token is None:
                return f"Could not find token at address {address}."

            name = token.get("name", "Unknown")
            symbol = token.get("symbol", "???")
            price = token.get("price") or token.get("currentPrice") or 0
            market_cap = token.get("marketCap") or token.get("market_cap") or 0
            description = token.get("description", "N/A")[:200]
            listed = token.get("listed") or token.get("isListed") or False

            report = self.generate_report(token, topic)
            return (
                f"**Research Report: {name} ({symbol})**\\n"
                f"Address: {address}\\n"
                f"Price: {float(price):.6f} FET | MC: {float(market_cap):,.0f} FET\\n"
                f"DEX listed: {'Yes' if listed else 'No'}\\n"
                f"Description: {description}\\n\\n"
                f"--- AI Analysis ({topic}) ---\\n"
                f"{report}"
            )

        return (
            f"Welcome to {BUSINESS['name']}!\\n\\n"
            f"Commands:\\n"
            f"  report <token_address>            — full research report\\n"
            f"  report <token_address> <topic>    — focused analysis\\n\\n"
            f"Example topics: overview, risks, tokenomics, competition\\n\\n"
            f"Powered by {BUSINESS['ai_model']}"
        )


# ==============================================================================
# REPLY HELPER
# ==============================================================================


async def reply(ctx: Context, sender: str, text: str, end: bool = False) -> None:
    content = [TextContent(type="text", text=text)]
    if end:
        content.append(EndSessionContent(type="end-session"))
    try:
        await ctx.send(
            sender,
            ChatMessage(timestamp=datetime.utcnow(), msg_id=uuid4(), content=content),
        )
    except Exception as e:
        ctx.logger.error(f"Failed to send reply to {sender[:20]}: {e}")


# ==============================================================================
# MAIN AGENT
# ==============================================================================

cache = Cache(max_size=1000)
security = Security()
health = Health()
revenue = Revenue(cache)
business = ResearchBusiness(cache)

agent = Agent()
chat_proto = Protocol(spec=chat_protocol_spec)


@chat_proto.on_message(ChatMessage)
async def handle_chat(ctx: Context, sender: str, msg: ChatMessage) -> None:
    try:
        await ctx.send(
            sender,
            ChatAcknowledgement(
                timestamp=datetime.utcnow(), acknowledged_msg_id=msg.msg_id
            ),
        )
    except Exception as e:
        ctx.logger.error(f"Failed to send ack to {sender[:20]}: {e}")

    text = " ".join(
        item.text for item in msg.content if isinstance(item, TextContent)
    ).strip()
    text = text[: BUSINESS["max_input_length"]]

    clean, error = security.check(ctx, sender, text)
    if error:
        health.record(False)
        await reply(ctx, sender, error, end=True)
        return

    Logger.audit(ctx, sender, "request")

    lower = clean.lower()

    if lower in ("help", "?"):
        tier = revenue.get_tier(sender)
        await reply(
            ctx,
            sender,
            f"**{BUSINESS['name']}** v{BUSINESS['version']}\\n\\n"
            f"{BUSINESS['description']}\\n\\n"
            f"Your tier: {tier.upper()}\\n\\n"
            f"Commands: help, status, tokenize, report <addr> [topic]",
        )
        return

    if lower == "status":
        s = health.status()
        await reply(
            ctx,
            sender,
            f"Status: {s['status']} | Uptime: {s['uptime_seconds']}s | "
            f"Requests: {s['requests']} | Error rate: {s['error_rate']}",
        )
        return

    if "tokenize" in lower:
        if OWNER_ADDRESS and sender != OWNER_ADDRESS:
            await reply(ctx, sender, "Only the agent owner can trigger tokenization.", end=True)
            return
        result = AgentLaunch.tokenize()
        link = result.get("data", {}).get("handoff_link") or result.get("handoff_link")
        await reply(
            ctx,
            sender,
            f"Token created! Deploy here: {link}" if link else f"Result: {json.dumps(result)}",
            end=True,
        )
        return

    tier = revenue.get_tier(sender)
    allowed, quota_error = revenue.check_quota(sender, tier)
    if not allowed:
        health.record(False)
        await reply(ctx, sender, quota_error, end=True)
        return

    try:
        response = await business.handle(ctx, sender, clean, tier)
        health.record(True)
    except Exception as e:
        health.record(False)
        Logger.error(ctx, "business_handle", str(e))
        response = "Something went wrong. Please try again."

    await reply(ctx, sender, response, end=True)


@chat_proto.on_message(ChatAcknowledgement)
async def handle_ack(ctx: Context, sender: str, msg: ChatAcknowledgement) -> None:
    ctx.logger.debug(f"Ack from {sender[:20]} for msg {msg.acknowledged_msg_id}")


@agent.on_interval(period=3600)
async def periodic_health(ctx: Context) -> None:
    ctx.logger.info(f"[HEALTH] {json.dumps(health.status())}")


agent.include(chat_proto, publish_manifest=True)

if __name__ == "__main__":
    agent.run()
`,
};
